# -*- coding: utf-8 -*-
"""Comprehensive_banking_analytics_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sa0g9H1yds4kJ57Q-wjY9lXAGe41j6ud
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import altair as alt

from imblearn.over_sampling import SMOTE

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

# importing model evaluation libraries
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression

import pandas as pd
from sklearn.linear_model import Lasso
from sklearn.feature_selection import mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

url = 'https://drive.usercontent.google.com/download?id=1clIQ8Rj1hOJz6F4tDj5iJCaRvxXyMABI&export=download&authuser=0&confirm=t&uuid=811b757b-2100-43f9-80aa-a0c10a52ad06&at=APZUnTWztJ89713JpEU-p0LG3dKo:1721408003556'
df = pd.read_csv(url)
df.head()

df.iloc[1]

df.info()

df.describe().T

df.describe(include = 'object').T

df['Delay_from_due_date'].value_counts()

df['Type_of_Loan'].unique()

df['Type_of_Loan'].nunique() #too many unique values in type of loan.

df['Credit_Mix'].unique()

df['Customer_ID'].nunique()

df['Name'].nunique()

df['Month'].unique()

df['Age'].unique()

"""Exploratory Analysis so far:

1) Dataset has 100000 rows and 28 columns

2) Out of 28 columns, 7 are categorical columns and rest are Numerical columns

3) There are 12500 unique values in the customer ID column, indicating that there is data of 12500 customers

4) There are 8 unique values in Month column, indicating that there is data of 12500 customers from Januray to August.

**DATA PRE-PROCESSING- Missing values, Outliers and Encoding**
"""

df.isnull().sum() #No misssing values

sns.countplot(y='Occupation', hue='Credit_Score', data=df).set(title='Credit Score based on Occcupation')

sns.countplot(y='Credit_Mix', hue='Credit_Score', data=df)

sns.countplot(y='Payment_of_Min_Amount', hue='Credit_Score', data=df)

sns.countplot(y='Payment_Behaviour', hue='Credit_Score', data=df)

sns.barplot(x='Annual_Income', y='Credit_Score', hue='Occupation', data=df).set(title='Credit score of different occupants based on their annual Income')

sns.barplot(x='Credit_Score', y='Monthly_Inhand_Salary', data=df).set(title='Credit Score based on Monthly Inhand Salary')

sns.barplot(x='Credit_Score', y='Num_Bank_Accounts', data=df).set(title='Credit Score based on Num of Bank Accounts')

sns.barplot(x='Credit_Score', y='Num_Credit_Card', data=df).set(title='Credit Score based on Num of Credit Cards')

sns.barplot(x='Credit_Score', y='Interest_Rate', data=df).set(title='Credit Score based on Interest_Rate')

sns.barplot(x='Credit_Score', y='Num_of_Loan', data=df).set(title='Credit Score based on Number of Loan')

sns.barplot(x='Credit_Score', y='Delay_from_due_date', data=df).set(title='Credit Score based on Delay_from_due_date')

sns.barplot(x='Credit_Score', y='Num_of_Delayed_Payment', data=df).set(title='Credit Score based on Num_of_Delayed_Payment')

sns.barplot(x='Credit_Score', y='Changed_Credit_Limit', data=df).set(title='Credit Score based on Changed_Credit_Limit')

sns.barplot(x='Credit_Score', y='Num_Credit_Inquiries', data=df).set(title='Credit Score based on Number of Credit_Inquiries')

sns.barplot(x='Credit_Score', y='Outstanding_Debt', data=df).set(title='Credit Score based on Outstanding_Debt')

sns.barplot(x='Credit_Score', y='Credit_Utilization_Ratio', data=df).set(title='Credit Score based on Credit_Utilization_Ratio')

sns.barplot(x='Credit_Score', y='Credit_History_Age', data=df).set(title='Credit Score based on Credit_History_Age')

sns.barplot(x='Credit_Score', y='Total_EMI_per_month', data=df).set(title='Credit Score based on Total_EMI_per_month')

sns.barplot(x='Credit_Score', y='Amount_invested_monthly', data=df).set(title='Credit Score based on Amount_invested_monthly')

sns.barplot(x='Credit_Score', y='Monthly_Balance', data=df).set(title='Credit Score based on Monthly_Balance')

"""**Boxplots to see Outliers**"""

data=df.copy()

# Set seaborn style directly
sns.set_style("white")

# Creating the figure with specified size
plt.figure(figsize=(15, 10))

# First subplot
ax1 = plt.subplot(341)
sns.boxplot(y=data['Monthly_Balance'], ax=ax1)
ax1.set_title('Monthly_Balance')

# Second subplot
ax2 = plt.subplot(342)
sns.boxplot(y=data['Amount_invested_monthly'], ax=ax2)
ax2.set_title('Amount_invested_monthly')

# Third subplot
ax3 = plt.subplot(343)
sns.boxplot(y=data['Total_EMI_per_month'], ax=ax3)
ax3.set_title('Total_EMI_per_month')

ax4 = plt.subplot(344)
sns.boxplot(y=data['Annual_Income'], ax=ax4)
ax4.set_title('Annual_Income')

ax5 = plt.subplot(345)
sns.boxplot(y=data['Monthly_Inhand_Salary'], ax=ax5)
ax5.set_title('Monthly_Inhand_Salary')

ax6 = plt.subplot(346)
sns.boxplot(y=data['Age'], ax=ax6)
ax6.set_title('Age')

ax7 = plt.subplot(347)
sns.boxplot(y=data['Monthly_Inhand_Salary'], ax=ax7)
ax7.set_title('Monthly_Inhand_Salary')

ax8 = plt.subplot(348)
sns.boxplot(y=data['Interest_Rate'], ax=ax8)
ax8.set_title('Interest_Rate')

ax9 = plt.subplot(349)
sns.boxplot(y=data['Outstanding_Debt'], ax=ax9)
ax9.set_title('Outstanding_Debt')

ax10 = plt.subplot(3,4,10)
sns.boxplot(y=data['Changed_Credit_Limit'], ax=ax10)
ax10.set_title('Changed_Credit_Limit')

ax11 = plt.subplot(3,4,11)
sns.boxplot(y=data['Credit_Utilization_Ratio'], ax=ax11)
ax11.set_title('Credit_Utilization_Ratio')


plt.tight_layout()
plt.show()

"""**OUTLIER CAPPING - Monthly_Balance, Amount_invested_monthly, Total_EMI_per_month, Annual_Income, Monthly_Inhand_Salary have significant outliers. Let's fix that by IQR**"""

numerical_features=['Monthly_Balance','Amount_invested_monthly','Total_EMI_per_month', 'Annual_Income', 'Monthly_Inhand_Salary','Outstanding_Debt']
for cols in numerical_features:
    Q1 = data[cols].quantile(0.25)
    Q3 = data[cols].quantile(0.75)
    IQR = Q3 - Q1

    filter = (data[cols] >= Q1 - 1.5 * IQR) & (data[cols] <= Q3 + 1.5 *IQR)
    data=data.loc[filter]

data['Monthly_Balance'].max()

# Set seaborn style directly
sns.set_style("white")

# Creating the figure with specified size
plt.figure(figsize=(15, 10))

# First subplot
ax1 = plt.subplot(341)
sns.boxplot(y=data['Monthly_Balance'], ax=ax1)
ax1.set_title('Monthly_Balance')

# Second subplot
ax2 = plt.subplot(342)
sns.boxplot(y=data['Amount_invested_monthly'], ax=ax2)
ax2.set_title('Amount_invested_monthly')

# Third subplot
ax3 = plt.subplot(343)
sns.boxplot(y=data['Total_EMI_per_month'], ax=ax3)
ax3.set_title('Total_EMI_per_month')

ax4 = plt.subplot(344)
sns.boxplot(y=data['Annual_Income'], ax=ax4)
ax4.set_title('Annual_Income')

ax5 = plt.subplot(345)
sns.boxplot(y=data['Monthly_Inhand_Salary'], ax=ax5)
ax5.set_title('Monthly_Inhand_Salary')

ax6 = plt.subplot(346)
sns.boxplot(y=data['Outstanding_Debt'], ax=ax6)
ax6.set_title('Outstanding_Debt')


# Display the plots
plt.tight_layout()
plt.show()

# Select only numeric columns
numeric_data = data.select_dtypes(include=['number'])

# Compute the correlation matrix on the numeric data
corr_matrix = numeric_data.corr()

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))  # Adjust the figure size as needed

# Use seaborn's heatmap to visualize the correlation matrix
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

# Add title
plt.title('Correlation Matrix')

# Show the plot
plt.show()

"""**ENCODING**"""

# Dropping columns that are not useful for clustering
columns_to_drop = ['ID', 'Customer_ID', 'Name', 'SSN', 'Type_of_Loan']
data_cleaned = data.drop(columns=columns_to_drop)

#ordinal encoding using map
data['Credit_Mix'] = data['Credit_Mix'].map({'Good':2, 'Standard':1, 'Bad':0})
data['Credit_Score'] = data['Credit_Score'].map({'Good':2, 'Standard':1, 'Poor':0})
data['Payment_of_Min_Amount'] = data['Payment_of_Min_Amount'].map({'No': 0, 'NM': 1, 'Yes': 2})

# One-Hot Encoding for 'Occupation' and 'Payment_Behaviour'
data = pd.get_dummies(data, columns=['Occupation'], prefix='Occ')
data = pd.get_dummies(data, columns=['Payment_Behaviour'], prefix='PayBehaviour')


#Display the first few rows of the updated dataset
data.head()

data['Credit_Score'].unique()

"""*Finally, all the features in the data are now numerical.*

**# Scaling Numerical Columns**

Standardization ensures that all features contribute equally to the clustering process, regardless of their original scales.
"""

from sklearn.preprocessing import StandardScaler

# Normalizing numerical features
numerical_columns = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',
                     'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment',
                     'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Outstanding_Debt',
                     'Credit_Utilization_Ratio', 'Credit_History_Age', 'Total_EMI_per_month',
                     'Amount_invested_monthly', 'Monthly_Balance']

scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Display the preprocessed data
data.head()

"""Feature reduction

Currently, the data contains several independent features, and they may all be useless for modeling as they may have a weak relationship with the target variable. Therefore, we will remove unwanted features and select the important ones.
We start by dropping columns irrelevant to building a credit-scoring model.
"""

data.drop(['ID', 'Customer_ID', 'SSN', 'Name','Type_of_Loan'], axis=1, inplace=True)
data

"""**1. CUSTOMER SEGMENTATION(CLUSTERING)**"""

# Determine the optimal number of clusters using the elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)

# Plotting the elbow method results
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Initialize an empty dictionary to store the results
results = {}

# Loop through each number of clusters
for n_clusters in [2, 3, 4, 5]:
    # Apply K-Means clustering
    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
    cluster_labels = kmeans.fit_predict(data)  # Using a subset of 5,000 samples

    # Calculate the silhouette score
    silhouette_avg = silhouette_score(data, cluster_labels)

    # Calculate the Davies-Bouldin Index
    davies_bouldin = davies_bouldin_score(data, cluster_labels)

    # Calculate the Calinski-Harabasz Index
    calinski_harabasz = calinski_harabasz_score(data, cluster_labels)

    # Store the results
    results[n_clusters] = {
        'Silhouette Score': silhouette_avg,
        'Davies-Bouldin Index': davies_bouldin,
        'Calinski-Harabasz Index': calinski_harabasz
    }

# Print the results
for n_clusters, metrics in results.items():
    print(f"Number of Clusters: {n_clusters}")
    print(f"  Silhouette Score: {metrics['Silhouette Score']}")
    print(f"  Davies-Bouldin Index: {metrics['Davies-Bouldin Index']}")
    print(f"  Calinski-Harabasz Index: {metrics['Calinski-Harabasz Index']}")
    print("\n")

"""Based on the typical interpretation of clustering validation metrics:

Silhouette Score: Higher values indicate better-defined clusters. This score reflects how similar a data point is to its own cluster compared to other clusters.

Davies-Bouldin Index: Lower values indicate better clustering quality. This index measures the average similarity ratio of each cluster with its most similar cluster.

Calinski-Harabasz Index: Higher values indicate better-defined clusters. This index measures the ratio of the sum of between-cluster dispersion and within-cluster dispersion.

In short, looking for:

The highest Silhouette Score.
The lowest Davies-Bouldin Index.
The highest Calinski-Harabasz Index.
"""

# Apply KMeans clustering with the optimal number of clusters determined from the elbow method (k=2)
optimal_k = 2
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
data['Cluster'] = kmeans.fit_predict(data)

# Display the first few rows with the cluster labels
data[['Annual_Income', 'Cluster']].head()

"""*The scatter plot shows the distribution of customers across the first two principal components, colored by their assigned clusters. However, to gain deeper insights into these customer segments, we need to analyze the characteristics of each cluster in terms of the original features.*"""

import seaborn as sns

# For each cluster, plot a box plot for key features
plt.figure(figsize=(14, 8))
sns.boxplot(x='Cluster', y='Annual_Income', data=data)
plt.title('Distribution of Annual Income Across Clusters')
plt.show()

sns.boxplot(x='Cluster', y='Credit_Utilization_Ratio', data=data)
plt.title('Distribution of Credit Utilization Ratio Across Clusters')
plt.show()

# Plot the count of a categorical feature within each cluster
plt.figure(figsize=(14, 8))
sns.countplot(x='Cluster', hue='Credit_Score', data=data)
plt.title('Credit Score Distribution Across Clusters')
plt.show()

sns.pairplot(data[['Annual_Income', 'Credit_Utilization_Ratio', 'Age', 'Cluster']], hue='Cluster')
plt.show()

cluster_means = data.groupby('Cluster').mean()
plt.figure(figsize=(12, 8))
sns.heatmap(cluster_means.T, cmap='viridis', annot=True)
plt.title('Cluster Feature Mean(s)')
plt.show()

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Step 1: Apply PCA to reduce the data to 2 dimensions
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data)
# Step 2: Apply K-Means Clustering with 2 clusters
kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)
cluster_labels = kmeans.fit_predict(data)

# Step 3: Plot the clusters
plt.figure(figsize=(10, 7))
plt.scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.7)
plt.title('Visualization of Clusters with 2 Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

"""*The scatter plot shows the distribution of customers across the first two principal components, colored by their assigned clusters. However, to gain deeper insights into these customer segments, we need to analyze the characteristics of each cluster in terms of the original features.*"""

# Group the data by clusters and calculate the mean of each feature within each cluster
cluster_profiles = data.groupby('Cluster').mean()

# Display the profiles of each cluster
print("Cluster Profiles:")
print(cluster_profiles)

# Optionally, if you want to focus on a few top features for each cluster:
top_features = ['Age', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Credit_Utilization_Ratio']

print("\nTop Features for Each Cluster:")
print(cluster_profiles[top_features])

"""To summarise:

Cluster 1 - represent customers who are financially stable, make timely payments, and have a good credit history.

Cluster 0 - represent customers who are more likely to delay payments, have higher outstanding debt, and may be at higher risk

**2. CREDIT RISK ASSESSMENT(CLASSIFICATION)**
"""

# importing key libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE, SelectKBest, f_classif

# extracting the predictor variables
X = data.drop(['Credit_Score', 'Cluster'], axis=1)

# extracting the target variable
y = data['Credit_Score']

# Method 1: finding feature importance using Tree-Based method
rf_model = RandomForestClassifier()
rf_model.fit(X, y)
feature_importances = rf_model.feature_importances_
top_10_rf = X.columns[feature_importances.argsort()[-10:][::-1]]

print("Top 10 variables from Tree-Based Method:", ', '.join(top_10_rf))
print("---------------------------------------------------------------------------------------------------------")

# # Method 2: using Recursive Feature Elimination (RFE)
rfe_selector = RFE(estimator=RandomForestClassifier(), n_features_to_select=10, step=1)
rfe_selector.fit(X, y)
top_10_rfe = X.columns[rfe_selector.support_]

print("Top 10 variables from Method Recursive Feature Elimination:", ', '.join(top_10_rfe))
print("---------------------------------------------------------------------------------------------------------")

# Method 3: using Univariate Feature Selection
selector = SelectKBest(score_func=f_classif, k=10)
selector.fit(X, y)
top_10_univariate = X.columns[selector.get_support()]

print("Top 10 variables from Univariate Feature Selection:", ', '.join(top_10_univariate))

"""*We find the important columns by picking only those that appear in at least one of the feature selection method’s outputs.*"""

imp_columns = list(set(top_10_rf.tolist() + top_10_rfe.tolist() + top_10_univariate.tolist()))
imp_columns
print('Number of selected columns are: ', len(imp_columns))

imp_columns

"""*Checking multicollinearity*

*Correlation matrix: We use correlation to check for multicollinearity and mark the columns where the absolute correlation value is above 0.7*
"""

# extracting the selected independent features
X_selected = data[imp_columns]

# creating correlation matrix
correlation_matrix = X_selected.corr()

# finding highly correlated feature pairs
highly_correlated_pairs = (correlation_matrix.abs() > 0.7) & (correlation_matrix.abs() < 1)

print("Highly correlated pairs of variables and their correlation values:\n")
checked_pairs = set() # To keep track of checked pairs
for col1 in X_selected.columns:
    for col2 in X_selected.columns:
        if col1 != col2 and (col1, col2) not in checked_pairs and (col2, col1) not in checked_pairs:
            if highly_correlated_pairs.loc[col1, col2]:
                correlation_value = correlation_matrix.loc[col1, col2]
                print(f"{col1} - {col2}: {correlation_value:.2f}")
                checked_pairs.add((col1, col2))

"""*We now remove the correlated columns.*"""

columns_to_remove = ['Credit_Mix', 'Annual_Income']
X_reduced = X_selected.drop(columns=columns_to_remove)
X_reduced

"""*Variance inflation factor (VIF): To eliminate multicollinearity, we will calculate the VIF value for each independent feature.*"""

# calculating vif ofthe selected features
vif_df = pd.DataFrame()
vif_df["Variable"] = X_reduced.columns
vif_df["VIF"] = [variance_inflation_factor(X_reduced.values, i) for i in range(X_reduced.shape[1])]
print("Variance Inflation Factors:")
print(vif_df)

print('Number of selected columns are: ', len(X_reduced.columns))

final_X_cols = X_reduced.columns.tolist()
print("Final selected predictors are: ", ', '.join(final_X_cols))

"""*We now create the final dataset for modeling with the 13 selected independent features and the target (dependent) variable.*"""

data_final = data[final_X_cols + ['Credit_Score']]
data_final

"""*Balancing data *"""

data['Credit_Score'].value_counts()

category_counts = data['Credit_Score'].value_counts()
plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%')
plt.axis('equal')
plt.title('Credit Score Distribution')
plt.show()

"""*We are dealing with class-imbalanced data, as evidenced by the number of categories in the target feature. *

**We used the SMOTE method, where synthetic samples are created for the minority classes to balance the classes.**
"""

# initializing SMOTE
smote = SMOTE()
# fitting SMOTE to the data
X_bal, y_bal = smote.fit_resample(X, y)
y_bal.value_counts()

category_counts = y_bal.value_counts()
plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%')
plt.axis('equal')
plt.title('Credit Score Distribution')
plt.show()

len(X_bal)

X_train, X_test, y_train, y_test = train_test_split(X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy_score(y_test, y_pred))
print('Classification Report \n', classification_report(y_test, y_pred))

"""SMOTE Application: SMOTE is applied only on the training data to balance the classes. This means that the test set still retains its original class distribution, which can lead to unequal support numbers in the classification report.

Stratified Splitting: using stratify=y_resampled in the train_test_split function, the test set will have the same proportion of classes as the original dataset before applying SMOTE.

**Default Payment Prediction**
"""

data_final['Credit_Score'].value_counts()

data_final = data_final.copy()  # Make sure it's a copy, not a view
data_final.loc[:, 'Default_Proxy'] = data_final['Credit_Score'].apply(lambda x: 0 if x in [0, 1] else 1)
data_final['Default_Proxy'] .value_counts()

data_final

print(y.shape)  # Should return (number_of_rows,)

print(X.shape)  # Should return (number_of_rows, num of columns)

X = data_final.drop(['Credit_Score', 'Default_Proxy'], axis=1)
y = data_final['Default_Proxy']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
sm = SMOTE(random_state=0)
X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)  #
model = RandomForestClassifier()
model.fit(X_train_bal, y_train_bal)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print('Classification Report \n', classification_report(y_test, y_pred))

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cross_val_results = cross_val_score(RandomForestClassifier(random_state=42),
                                    X_train_bal, y_train_bal, cv=skf, scoring='accuracy')

print(cross_val_results)
print("Mean Accuracy: ", cross_val_results.mean())

# Assuming y_test and y_pred are defined
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

# AUC-ROC
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class
auc_score = roc_auc_score(y_test, y_pred_proba)
print("AUC-ROC:", auc_score)

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (area = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""When using SMOTE or any other resampling technique, the proper sequence is crucial to prevent data leakage. Here’s how you should handle it:

Correct Order:
Train-Test Split First:
Reason: Splitting your data into training and test sets first ensures that the test set remains untouched by any transformations applied to the training data. This preserves the integrity of the evaluation process, ensuring that your model is evaluated on data it has never seen before.
Apply SMOTE on the Training Set Only:
Reason: SMOTE should only be applied to the training data. The purpose of SMOTE is to help the model learn from a balanced dataset during training. Applying SMOTE before the split would cause the same synthetic data to be present in both the training and test sets, leading to overfitting and misleadingly high performance metrics.
Summary:
Step 1: Perform train_test_split to separate your data into training and test sets.
Step 2: Apply SMOTE (or any other resampling technique) only to the training set.
Step 3: Train your model on the balanced training set.
Step 4: Evaluate your model on the untouched test set.
This approach ensures that your model's performance is evaluated fairly and accurately.

Would you like to proceed with any further steps based on this approach?

**PERFORMANCE PREDICTION(REGRESSION):**
"""

ds = df.copy()

#Adding some new features
ds['Interest_Revenue'] = ds['Outstanding_Debt'] * ds['Interest_Rate']
ds['EMI_to_Salary_Percentage'] = (ds['Total_EMI_per_month'] / (ds['Monthly_Inhand_Salary'] + 1e-9) * 100)
ds['Debt_to_Income_Ratio'] = (ds['Outstanding_Debt'] / (ds['Annual_Income'] + 1e-9) * 100)

# Purpose of Adding 1e-9:
# Preventing Division by Zero: If any customer in the dataset has an Annual_Income of zero, dividing by this value would result in a
# division by zero error, which would crash the code or result in undefined values (infinity). Adding a tiny value like 1e-9 ensures that
# the division always happens with a non-zero denominator.
# Negligible Impact: The value 1e-9 (0.000000001) is so small that it has a negligible impact on the resulting ratio.
# It ensures the operation is safe without significantly altering the computed ratio.


#Encoding
ds['Credit_Mix'] = ds['Credit_Mix'].map({'Good':2, 'Standard':1, 'Bad':0})
ds['Credit_Score'] = ds['Credit_Score'].map({'Good':2, 'Standard':1, 'Poor':0})
ds['Payment_of_Min_Amount'] = ds['Payment_of_Min_Amount'].map({'No': 0, 'NM': 1, 'Yes': 2})

ds = pd.get_dummies(ds, columns=['Occupation'], prefix='Occ')
ds = pd.get_dummies(ds, columns=['Payment_Behaviour'], prefix='PayBehaviour')

#Removing unnecessary columns
ds.drop(['ID',	'Customer_ID',	'Month',	'Name', 'SSN', 'Type_of_Loan'], axis=1, inplace=True)

# Define the feature set and the target variable
X = ds.drop(columns=['Interest_Revenue', 'Outstanding_Debt', 'Interest_Rate'])  # Exclude 'Outstanding_Debt' and 'Interest_Rate'
y = ds['Interest_Revenue']

#Feature Selection
# 1. Mutual Information
mi_scores = mutual_info_regression(X, y)
mi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores}).sort_values(by='MI Score', ascending=False)
mi_selected_features = mi_df.head(10)['Feature']
mi_selected_features.to_list()
print("Top 10 variables from Mutual Information Method:", ', '.join(mi_selected_features))
print("---------------------------------------------------------------------------------------------------------")

# 2. Correlation Matrix
correlation_matrix = ds.corr()
corr_threshold = 0.5  # Define a threshold for correlation
corr_selected_features = correlation_matrix['Interest_Revenue'].abs()
corr_selected_features = corr_selected_features[corr_selected_features > corr_threshold].index.drop(['Interest_Revenue', 'Outstanding_Debt', 'Interest_Rate'])
corr_selected_features.to_list()
print("Top 10 variables from Correlation Matrix:", ', '.join(corr_selected_features))
print("---------------------------------------------------------------------------------------------------------")

# 3. Lasso Regression
lasso = Lasso(alpha=0.1, max_iter=10000, random_state=42)
lasso.fit(X, y)
# Get the coefficients
coefficients = pd.Series(lasso.coef_, index=X.columns)
# Select the top 10 features based on the absolute value of their coefficients
top_10_features = coefficients.abs().sort_values(ascending=False).head(10).index.tolist()
# Display the top 10 features
print("Top 10 variables from Lasso Regression:", ', '.join(top_10_features))

important_features = list(set(top_10_features + mi_selected_features.tolist() + corr_selected_features.tolist()))
important_features

print('Number of selected columns are: ', len(important_features))

"""*Checking multicollinearity*"""

# extracting the selected independent features
X_selected = ds[important_features]

# creating correlation matrix
correlation_matrix = X_selected.corr()

# finding highly correlated feature pairs
highly_correlated_pairs = (correlation_matrix.abs() > 0.7) & (correlation_matrix.abs() < 1)

print("Highly correlated pairs of variables and their correlation values:\n")
checked_pairs = set() # To keep track of checked pairs
for col1 in X_selected.columns:
    for col2 in X_selected.columns:
        if col1 != col2 and (col1, col2) not in checked_pairs and (col2, col1) not in checked_pairs:
            if highly_correlated_pairs.loc[col1, col2]:
                correlation_value = correlation_matrix.loc[col1, col2]
                print(f"{col1} - {col2}: {correlation_value:.2f}")
                checked_pairs.add((col1, col2))

#remove Credit_Mix, Annual_Income, Amount_invested_monthly to remove multi-colinearity
features_to_remove = ['Credit_Mix', 'Annual_Income', 'Amount_invested_monthly']
X_reduced = X_selected.drop(columns=features_to_remove)

# Convert Boolean columns to integers
X_final = X_reduced.apply(lambda col: col.astype(int) if col.dtype == 'bool' else col)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X_reduced.columns
vif_data["VIF"] = [variance_inflation_factor(X_final.values, i) for i in range(X_final.shape[1])]

# Display VIF values
print(vif_data)

#selecting features and target variable
X_no_multi = X_reduced
y = ds['Interest_Revenue']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_no_multi)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
r2 = r2_score(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)

# Print the evaluation metrics
print("R2 Score:", r2)
print("RMSE:", rmse)
print("MAE:", mae)

#Doing Cross-Validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
print("Cross-validated R2 scores:", cv_scores)
print("Average R2 score:", cv_scores.mean())

